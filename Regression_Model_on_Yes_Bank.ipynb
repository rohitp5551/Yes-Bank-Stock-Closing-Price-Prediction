{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "lqI4ATsU9Egv",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "578E2V7j08f6"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohitp5551/Yes-Bank-Stock-Closing-Price-Prediction/blob/main/Regression_Model_on_Yes_Bank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Name**            - Rohit Patil\n",
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project focused on predicting stock prices using historical data. The dataset underwent thorough cleaning to handle missing values, correct data types, and manage outliers, ensuring data quality. Categorical variables were encoded, and numerical features were scaled for optimal modeling.\n",
        "\n",
        "A Ridge regression model was selected for its ability to manage multicollinearity and prevent overfitting. Hyperparameter tuning using GridSearchCV identified the best model with an optimal alpha value of 0.1.\n",
        "\n",
        "The model demonstrated strong performance during evaluation on both training and test datasets. It achieved a high R-squared score of 0.98 before and 0.97 after hyperparameter tuning.Mean squared error (MSE) metrics were also low, with values of 39.03 before and 55.98 after tuning, highlighting the model's predictive accuracy.\n",
        "\n",
        "Exploratory Data Analysis (EDA) provided insights into stock price trends and relationships between variables. Visualizations such as line plots and scatter plots facilitated understanding of data patterns and influential factors affecting stock prices.\n",
        "\n",
        "In summary, the project successfully applied machine learning techniques, specifically Ridge regression, to predict stock prices. Rigorous data preprocessing, effective model selection through hyperparameter tuning, and comprehensive evaluation metrics underscored the project's methodology and findings. Future work could explore additional models or advanced feature engineering techniques to further enhance predictive capabilities."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/rohitp5551/Yes-Bank-Stock-Closing-Price-PredictionProvide"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project aims to predict stock prices using historical data, focusing on developing a machine learning model that accurately forecasts stock prices based on features like Open, High, Low, and Close. Key tasks include through data cleaning, preprocessing (including categorical encoding and numerical scaling), testing multiple models (such as linear,lasso,Ridge regression), and optimizing model performance through hyperparameter tuning. Evaluation will be based on metrics like R-squared(R2),Root mean sqaure error(RMSE),mean absolute error(MAE),mean squared error (MSE), with insights from Exploratory Data Analysis (EDA) informing model selection and feature engineering decisions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression,Lasso,Ridge\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score,root_mean_squared_error,accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bnekVMSuGsAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df=pd.read_csv('/content/drive/MyDrive/module 6/data_YesBank_StockPrices.csv')\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Dataset is about Yes bank monthly stock price which contains 185 rows with 5 columns.There is no duplicate and missing values in the dataset expect one incorrect datatype i.e in Date columns which is an object type ."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Date: Date of the record denoting the time period of stock in month and year format\n",
        "\n",
        "*   Open: Opening price of the stock\n",
        "*   High: Highest price of the stock in a day\n",
        "\n",
        "\n",
        "*   Low: Lowest price of the stock in a day\n",
        "\n",
        "\n",
        "*   Close: Closing price of the stock\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking Outliers\n",
        "sns.boxplot(data=df[[\"Open\",\"High\",\"Low\",\"Close\"]])\n",
        "plt.title(\"Box Plot of Open,High,Low,Close\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KDaAYUm4_31q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "df['Date']=pd.to_datetime(df['Date'],format='%b-%y')\n",
        "df[\"Month_number\"]=df[\"Date\"].dt.month\n",
        "df[\"Year\"]=df[\"Date\"].dt.year\n",
        "df.drop(\"Date\",axis=1,inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Converted Date column into Datetime datatype\n",
        "*   Extracted Month and year numbers from Date column\n",
        "\n",
        "*   After Extracting month and year drop the Date column\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Chart-1 Distribution of monthly price over Year**"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.lineplot(x=\"Month_number\",y=\"Close\",hue=\"Year\",data=df,marker=\"o\",palette='hls')\n",
        "plt.title(\"Distribution of price monthly\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line plot was chosen to visualize the monthly closing prices of stocks over different years. It helps in understanding how the closing prices fluctuate month by month across multiple years, highlighting trends and patterns over time."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the chart, insights can include identifying seasonal trends in stock prices. For example, consistent peaks or dips in certain months across multiple years could indicate recurring market behaviors. It also shows if there are any outlier months where prices significantly deviate from the general trend."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Yes, these insights can be valuable for strategic decision-making in trading or investment. Understanding seasonal patterns can help in timing buy or sell decisions, optimizing portfolio management, and managing risk more effectively.\n",
        "*   Insights indicating negative growth might include prolonged periods of declining closing prices across all or specific months over the years. This trend could imply economic downturns, sector-specific issues, or company-specific challenges affecting stock performance negatively. Identifying such trends early can prompt proactive measures to mitigate risks or adjust investment strategies accordingly.\n",
        "\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Chart-2 Comparing Average high and low price over year**"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "avg_prices_by_year = df.groupby('Year').agg({'High': 'mean', 'Low': 'mean'})\n",
        "\n",
        "# Create a bar chart to compare the average high and low prices over the years\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(avg_prices_by_year.index, avg_prices_by_year['High'], label='Average High Price', alpha=0.7)\n",
        "plt.bar(avg_prices_by_year.index, avg_prices_by_year['Low'], label='Average Low Price', alpha=0.7)\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Average Price')\n",
        "plt.title('Comparison of Average High and Low Prices Over Years')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a bar chart to compare the average high and low prices over the years because it visually represents the price range for each year effectively, enabling us to identify potential trends or patterns in the stock's price movement during those years.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the chart, we can infer whether the overall price volatility has increased or decreased over time. It also allows us to observe if the difference between the average high and average low prices has widened or narrowed, which can be an indication of increased or decreased market uncertainty or risk.\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " * Yes, insights from this chart can help businesses and investors identify periods of heightened volatility, which can be crucial for making informed trading or investment decisions.\n",
        " * Insights about increased volatility can signify higher risk and might lead to negative growth. For example, significant widening of the average high and low prices over time might suggest increasing market instability or an overall decline in the company's performance. This could cause investor confidence to decrease, leading to potential negative growth and a decline in the company's value.\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Chart-3 Relation Between Open and Close over Year**"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "for year in df['Year'].unique():\n",
        "    year_data = df[df['Year'] == year]\n",
        "    plt.scatter(year_data['Open'], year_data['Close'], label=f'Year {year}')\n",
        "plt.xlabel('Open Price')\n",
        "plt.ylabel('Close Price')\n",
        "plt.title('Relationship Between Open and Close Prices by Year')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plot showing the relationship between open and close prices by year was chosen because it visually represents how these two key price points correlate and vary across different years. It helps in understanding if there's a consistent pattern or trend in how stocks open and close over time."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights from the chart include identifying if there's a strong linear relationship between open and close prices each year. It helps in spotting any outliers where stocks opened significantly higher or lower compared to their closing prices, indicating intraday volatility or market sentiment shifts."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Yes, these insights are crucial for traders and investors in making informed decisions. Understanding the relationship between open and close prices can assist in predicting price movements throughout the trading day, optimizing entry or exit points, and managing risk more effectively.\n",
        "*   Insights indicating negative growth might include years where there's a noticeable divergence between open and close prices, especially if closing prices consistently fall below opening prices across multiple years. This divergence could signify bearish market conditions, economic downturns, or company-specific challenges affecting stock performance negatively. Recognizing such trends allows stakeholders to adjust strategies, hedge risks, or explore alternative investment opportunities during periods of market decline.\n",
        "\n"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Chart - 4 Closing and opening Price over Year**"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "avg_close_per_year=df.groupby(\"Year\").agg({\"Close\":\"mean\"})\n",
        "avg_open_per_year=df.groupby(\"Year\").agg({\"Open\":\"mean\"})\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(avg_close_per_year,marker=\"o\")\n",
        "plt.plot(avg_open_per_year,marker=\"o\")\n",
        "plt.title(\"Closing and Opening Price Over Year\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Closing Price\")\n",
        "plt.legend([\"Closing Price\",\"Opening Price\"])\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " Line plots are preferred in the above chart because they effectively depict the trend and fluctuations of average closing and opening prices over the years.\n",
        " The line plot provides a clear visualization of how the prices changed from year to year, highlighting the overall trend and any significant changes in average values.\n"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart visualize us the trend of closing and opening price of stock over different year.Significant changes in the trend of the average closing and opening prices can signal potential turning points in the stock's performance.\n",
        "These might indicate shifts in market conditions or changes in investor sentiment.\n",
        "\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart allows us to visualize the general direction of the stock's performance over time.\n",
        "It helps in identifying if the stock is experiencing long-term growth, decline, or stagnation.\n",
        "These insights are important for investment decisions, resource allocation, and overall business strategy.\n",
        "The chart allows us to visualize the general direction of the stock's performance over time.\n",
        "It helps in identifying if the stock is experiencing long-term growth, decline, or stagnation.\n",
        "These insights are important for investment decisions, resource allocation, and overall business strategy."
      ],
      "metadata": {
        "id": "vE9_0x6Qyf7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Chart-5 Correlation**"
      ],
      "metadata": {
        "id": "lqI4ATsU9Egv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "sns.heatmap(data=df.corr(),annot=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a correlation heatmap to visualize the relationships between numerical variables in the dataset. It helps in identifying patterns of linear correlation between different features (Open, High, Low, Close, Month_number, Year). This information is crucial for feature selection in predictive models and understanding how variables are interconnected.\n"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmap indicates the strength and direction of the linear correlation between each pair of features. For example, we can observe if \"Open\" and \"Close\" have a strong positive correlation, meaning that when one increases, the other tends to increase as well. It also shows if there's a negative correlation, meaning that as one variable increases, the other tends to decrease.\n"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Chart-6 Pair Plot**"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "plt.figure(figsize=(4,4))\n",
        "sns.pairplot(data=df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a pair plot because it provides a comprehensive overview of the relationships between multiple numerical variables in the dataset in a single visualization. It creates a matrix of scatter plots for each pair of variables, displaying their distribution and potential correlation. This helps in quickly spotting potential trends, patterns, and outliers across different feature combinations.\n"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pair plot helps in understanding the relationship between every combination of numerical features in the dataset. For example, it can visualize if there is a linear relationship between \"Open\" and \"Close\" or if \"High\" and \"Low\" tend to move together. It also helps in identifying if there are any outliers in specific features or combinations of features.\n"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "columns=[\"Open\",\"High\",\"Low\",\"Close\"]\n",
        "for i in columns:\n",
        "  q3=df[i].quantile(0.75)\n",
        "  q1=df[i].quantile(0.25)\n",
        "  iqr=q3-q1\n",
        "  upper_limit=q3+1.5*iqr\n",
        "  lower_limit=q1-1.5*iqr\n",
        "  df=df[(df[i]<=upper_limit) & (df[i]>lower_limit)]\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.boxplot(data=df[[\"Open\",\"High\",\"Low\",\"Close\"]])\n",
        "plt.title(\"Box Plot of Open,High,Low,Close\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G7kc6axa9BVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "t7BHHv76bmfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IQR Method**\n",
        "\n",
        "The technique used here is called Interquartile Range (IQR) outlier removal. It involves calculating the IQR, which is the difference between the third quartile (Q3) and the first quartile (Q1) of a dataset. Outliers are identified as values that fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR. In this function, for each specified column (Open, High, Low, Close), values outside this range are replaced with NaN. Finally, rows containing NaN values are dropped from the dataset. This method effectively filters out extreme values that skew statistical analysis or model performance based on their deviation from the dataset's central tendency."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=df.drop(columns=\"Close\")\n",
        "y=df[\"Close\"]\n"
      ],
      "metadata": {
        "id": "jHz0iGPZbT9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "id": "iej9UQ_vbfZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "v75q418ybfV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss=StandardScaler()\n",
        "x_trans=ss.fit_transform(x)\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_trans"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gBansFTj82VR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The method used here is StandardScaler from sklearn.preprocessing. StandardScaler scales the input data such that each feature has a mean of 0 and a standard deviation of 1. This transformation is crucial in machine learning to ensure that all features contribute equally to model training and prediction. Standardizing the data removes the mean and scales each feature to unit variance, which is particularly beneficial for algorithms that assume normally distributed data or require standardized inputs, such as linear regression, logistic regression, and support vector machines. It helps in improving the convergence rate and performance of these algorithms by reducing the impact of differing scales among features."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "x_train,x_test,y_train,y_test=train_test_split(x_trans,y,test_size=0.2,random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data splitting ratio used here is 80% training data and 20% testing data,\n",
        "specified by test_size=0.2 in train_test_split function. This ratio is commonly chosen to allocate a significant portion of the data for training the machine learning model (80%), while reserving a smaller portion for evaluating its performance (20%).\n",
        "\n",
        "The rationale behind this ratio is to ensure that the model learns patterns and relationships from a sufficiently large dataset during training, which helps in achieving better generalization and performance on unseen data. The test set serves as an independent dataset to assess how well the model can generalize to new, unseen data points, thus providing a measure of its predictive capability and robustness. This approach helps in detecting overfitting and ensures that the model's performance estimates are reliable."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape,y_train.shape"
      ],
      "metadata": {
        "id": "WDK48tYacail"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No,the dataset is not imbalanced as input data as well as output data contains the same number of rows."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 (Linear Regression)"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "lr=LinearRegression()\n",
        "\n",
        "# Fit the Algorithm\n",
        "lr.fit(x_train,y_train)\n",
        "# Predict on the model\n",
        "y_test_pred=lr.predict(x_test)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking Score of the model\n",
        "lr.score(x_test,y_test)*100"
      ],
      "metadata": {
        "id": "YLewq9KYdE1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr.coef_"
      ],
      "metadata": {
        "id": "8svaeQSq1aWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(9,5))\n",
        "plt.bar(x.columns,lr.coef_)\n",
        "plt.title(\"Linear Regression\")\n",
        "plt.xlabel(\"Columns\")\n",
        "plt.ylabel(\"Coefficient\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KVeU5tS412Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample1=np.array([[13.48,14.87,12.27,9,2005]])\n",
        "sample2=np.array([[13.20,14.47,12.40,10,2005]])\n",
        "sample_t1=ss.transform(sample1)\n",
        "sample_t2=ss.transform(sample2)\n",
        "print(lr.predict(sample_t1))\n",
        "print(lr.predict(sample_t2))"
      ],
      "metadata": {
        "id": "5Xdl272seEv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.plot(y_test.values,label=\"Actual_price\")\n",
        "plt.plot(y_test_pred,label=\"Predicted_price\")\n",
        "plt.title(\"Actual vs Predicted Price\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Evaluation"
      ],
      "metadata": {
        "id": "QpENcC8MbGoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of model on training and testing data\n",
        "\n",
        "mse_test=mean_squared_error(y_test,y_test_pred)\n",
        "rmse_test=root_mean_squared_error(y_test,y_test_pred)\n",
        "mae_test=mean_absolute_error(y_test,y_test_pred)\n",
        "r_square_test=r2_score(y_test,y_test_pred)\n",
        "\n",
        "evaluation_df=pd.DataFrame({\"Metric\":[\"MSE\",\"RMSE\",\"MAE\",\"R2\"],\"Test Values\":[mse_test,rmse_test,mae_test,r_square_test]})\n",
        "evaluation_df"
      ],
      "metadata": {
        "id": "bwmLxlgBbGM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "sZZfI2BLO2bK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Validation\n",
        "kfold=KFold(n_splits=5,shuffle=True,random_state=42)\n",
        "\n",
        "\n",
        "cv=cross_val_score(lr,x,y,cv=kfold)\n",
        "print(f\"Cross Validation accuracies of linear regression = {cv}\")\n",
        "\n",
        "mean_accuracy=round(sum(cv)/len(cv)*100,2)\n",
        "print(f\"Mean accuracy of Linear Regression={mean_accuracy}\")"
      ],
      "metadata": {
        "id": "kbJoiPL6O5Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "GiP_CE2_PBxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Linear Regression model, on single train test split we get the score 97.34% where as in Cross validation we get 98.47%.\n",
        "Cross validation is more reliable than train test split.\n",
        "In K fold Cross validation we split our data into 'k' no. of subsets .One chunk is used as test data for evaluation and remaining part is used to train the model but each time a different chunk will be used as test data.It test the model on various parts of data helping to trust that it will work well on unseen data."
      ],
      "metadata": {
        "id": "hs57H3OieXrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 Lasso Regression"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "la=Lasso()\n",
        "la.fit(x_train,y_train)\n",
        "la.score(x_test,y_test)*100"
      ],
      "metadata": {
        "id": "U6LMy9vB4sIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred1=la.predict(x_test)"
      ],
      "metadata": {
        "id": "MdlxbybS4sFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred1"
      ],
      "metadata": {
        "id": "v74W90aK4r18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "la.coef_"
      ],
      "metadata": {
        "id": "8iuZnSrW4ry4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(x.columns,la.coef_)\n",
        "plt.title(\"Lasso Regression\")\n",
        "plt.xlabel(\"Columns\")\n",
        "plt.ylabel(\"Coefficient\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zmKG6Ub54rwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample3=np.array([[16.20,20.95,16.02,3,2006]])\n",
        "sample4=np.array([[20.56,20.80,18.02,4,2006]])\n",
        "sample_t3=ss.transform(sample3)\n",
        "sample_t4=ss.transform(sample4)\n",
        "print(la.predict(sample_t3))\n",
        "print(la.predict(sample_t4))"
      ],
      "metadata": {
        "id": "3laNGBqC98k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(y_test.values,label=\"Actual Price\")\n",
        "plt.plot(y_pred1,label=\"Predicted Price\")\n",
        "plt.title(\"Actual vs Predicted Price\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Evaluation"
      ],
      "metadata": {
        "id": "J_lSeudLhgOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of model on test data\n",
        "mse1=mean_squared_error(y_test,y_pred1)\n",
        "rmse1=root_mean_squared_error(y_test,y_pred1)\n",
        "mae1=mean_absolute_error(y_test,y_pred1)\n",
        "r_square1=r2_score(y_test,y_pred1)\n",
        "\n",
        "evaluation_df_lasso=pd.DataFrame({\"Metric\":[\"MSE\",\"RMSE\",\"MAE\",\"R2\"],\"Test Values\":[mse1,rmse1,mae1,r_square1]})\n",
        "evaluation_df_lasso"
      ],
      "metadata": {
        "id": "mQX_OOGxhsOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "q_dc4tKAgUAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Validation\n",
        "kfold=KFold(n_splits=5,shuffle=True,random_state=42)\n",
        "cv2=cross_val_score(la,x,y,cv=kfold,)\n",
        "print(f\"Cross Validation accuracies of Lasso regression = {cv2}\")\n",
        "\n",
        "mean_accuracy2=round(sum(cv2)/len(cv2)*100,2)\n",
        "print(f\"Mean accuracy of Lasso regression = {mean_accuracy2}\")"
      ],
      "metadata": {
        "id": "itrPsM8LpAGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "la.get_params()"
      ],
      "metadata": {
        "id": "4kv7oscOgd0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "lwNFWDn7hJXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameter={\"alpha\":[0.01,0.1,1,10,100]}"
      ],
      "metadata": {
        "id": "9F8Drs4KhJ5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gs=GridSearchCV(la,parameter,cv=5,scoring=\"r2\")"
      ],
      "metadata": {
        "id": "5Q3ppQCPlMmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gs.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "9aRyDFVylU5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params=gs.best_params_\n",
        "best_params"
      ],
      "metadata": {
        "id": "qdft6hmRlZs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "highest=gs.best_score_\n",
        "highest_score=(highest)*100\n",
        "highest_score"
      ],
      "metadata": {
        "id": "auNGtVSIlrl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_best_estimator=gs.best_estimator_"
      ],
      "metadata": {
        "id": "fSbq0aBbYZ1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred_gs=model_best_estimator.predict(x_test)"
      ],
      "metadata": {
        "id": "0GvJNtYnYoqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred_gs"
      ],
      "metadata": {
        "id": "H85P1_4JY5GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?\n"
      ],
      "metadata": {
        "id": "VFPb5ZD4gvoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter optimization technique used here is GridSearchCV.It checks all the possible values of predefined hyperparameter and gets the optimum set of values that gives the highest accuracy.It is particularly effective when the number of the hyperparameters to tune is small and grid size is manageable,as it provide a comprehensive search over a limited space."
      ],
      "metadata": {
        "id": "drwemmH2g3yU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "GiDHEEffgeuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Lasso regression after train test split we get 98.75% as score of the model where as after tuning the model we get 98.58% score."
      ],
      "metadata": {
        "id": "DKjx6CvTzyrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse_test_gs=mean_squared_error(y_test,y_test_pred_gs)\n",
        "rmse_test_gs=root_mean_squared_error(y_test,y_test_pred_gs)\n",
        "mae_test_gs=mean_absolute_error(y_test,y_test_pred_gs)\n",
        "r2_test_gs=r2_score(y_test,y_test_pred_gs)\n",
        "\n",
        "evaluation_df_lasso_gs=pd.DataFrame({\"Metrics\":[\"MSE\",\"RMSE\",\"MAE\",\"R2\"],\"Test Values\":[mse_test_gs,rmse_test_gs,mae_test_gs,r2_test_gs]})\n",
        "evaluation_df_lasso_gs"
      ],
      "metadata": {
        "id": "3bXMH2m6oV6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 Ridge Regression"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "rd=Ridge()\n",
        "\n",
        "# Fit the Algorithm\n",
        "rd.fit(x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred2=rd.predict(x_test)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rd.score(x_test,y_test)*100"
      ],
      "metadata": {
        "id": "YRPhuvD6jxaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rd.coef_"
      ],
      "metadata": {
        "id": "hMn57wJoV6w6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(x.columns,rd.coef_)\n",
        "plt.title(\"Ridge Regression\")\n",
        "plt.xlabel(\"Columns\")\n",
        "plt.ylabel(\"Coefficient\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XYvwMzIhWE_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample5=np.array([[15.90,18.60,15.70,8,2006]])\n",
        "sample6=np.array([[18.00,18.88,16.80,9,2006]])\n",
        "sample_t5=ss.transform(sample5)\n",
        "sample_t6=ss.transform(sample6)\n",
        "print(rd.predict(sample_t5))\n",
        "print(rd.predict(sample_t6))"
      ],
      "metadata": {
        "id": "RFeUhVoKWE0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Evaluation"
      ],
      "metadata": {
        "id": "_V0QK7j4q_MB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of the model on test data\n",
        "\n",
        "mse_rd=mean_squared_error(y_test,y_pred2)\n",
        "rmse_rd=root_mean_squared_error(y_test,y_pred2)\n",
        "mae_rd=mean_absolute_error(y_test,y_pred2)\n",
        "r2_rd=r2_score(y_test,y_pred2)\n",
        "\n",
        "evaluation_df_rd=pd.DataFrame({\"Metric\":[\"MSE\",\"RMSE\",\"MAE\",\"R2\"],\"Test Values\":[mse_rd,rmse_rd,mae_rd,r2_rd]})\n",
        "evaluation_df_rd"
      ],
      "metadata": {
        "id": "6Yol88tMrINA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "nlH2k60TzsGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Validation\n",
        "kfold=KFold(n_splits=5,shuffle=True,random_state=42)\n",
        "cv3=cross_val_score(rd,x,y,cv=kfold,scoring=\"r2\")\n",
        "print(f\"Cross validation accuracies of Ridge Regression = {cv3}\")\n",
        "\n",
        "mean_accuracy3=round(sum(cv3)/len(cv3)*100,2)\n",
        "print(f\"Mean Accuracy of Ridge model= {mean_accuracy3}\")"
      ],
      "metadata": {
        "id": "ZQdtng210RsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter Tuning\n",
        "rd.get_params()"
      ],
      "metadata": {
        "id": "sz1lD2oO1y77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gs1=GridSearchCV(rd,parameter,cv=5,scoring=\"r2\")"
      ],
      "metadata": {
        "id": "8lGOqJOl2l6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gs1.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "-MEvPlIS2l3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params1=gs1.best_params_\n",
        "best_params1"
      ],
      "metadata": {
        "id": "jlmNu5a13HzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gs1.best_score_"
      ],
      "metadata": {
        "id": "c2yvhFlm3Hub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gs1.best_params_"
      ],
      "metadata": {
        "id": "JOwlkqQsuCIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_best_estimator_rd=gs1.best_estimator_\n"
      ],
      "metadata": {
        "id": "KpqNFZ8ulxRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred_gs1=model_best_estimator_rd.predict(x_test)"
      ],
      "metadata": {
        "id": "Mmsfm5illxOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred_gs1"
      ],
      "metadata": {
        "id": "HCMGXB8SlxLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?\n"
      ],
      "metadata": {
        "id": "4SyQd7oUzp0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter optimization technique used here is GridSearchCV.It checks all the possible values of predefined hyperparameter and gets the optimum set of values that gives the highest accuracy.It is particularly effective when the number of the hyperparameters to tune is small and grid size is manageable,as it provide a comprehensive search over a limited space.\n",
        "\n"
      ],
      "metadata": {
        "id": "mn-K1EYbo5nn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Tx2Ad6X60h_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Ridge Model after train test split,the score of the model is 98.28% where as after hyperparameter tuning the score of the model is 98.61%."
      ],
      "metadata": {
        "id": "h6W9lH8E0jiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse_rd1=mean_squared_error(y_test,y_test_pred_gs1)\n",
        "rmse_rd1=root_mean_squared_error(y_test,y_test_pred_gs1)\n",
        "mae_rd1=mean_absolute_error(y_test,y_test_pred_gs1)\n",
        "r2_rd1=r2_score(y_test,y_test_pred_gs1)\n",
        "\n",
        "evaluation_df_rd1=pd.DataFrame({\"Metric\":[\"MSE\",\"RMSE\",\"MAE\",\"R2\"],\"Test Values\":[mse_rd1,rmse_rd1,mae_rd1,r2_rd1]})\n",
        "evaluation_df_rd1"
      ],
      "metadata": {
        "id": "bt0Z7-CKpnbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(y_test.values,label=\"Actual Price\")\n",
        "plt.plot(y_pred2,label=\"Predicted Price\")\n",
        "plt.title(\"Actual vs Predicted Price\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Selection"
      ],
      "metadata": {
        "id": "hXl6yPT90m0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Metrics Considered: Mean Squared Error (MSE) and R-squared (R2).\n",
        "\n",
        "*   Reasoning: MSE measures the average squared difference between predicted values and actual values, providing insight into prediction accuracy. R-squared (R2) indicates the proportion of the variance in the dependent variable that is predictable from the independent variables, demonstrating how well the model fits the data. These metrics are crucial for assessing model performance and ensuring accurate predictions, which are essential for making informed business decisions.\n",
        "\n"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Chosen Model: Ridge Regression (Ridge model after hyperparameter tuning).\n",
        "\n",
        "*   Reasoning: Ridge regression was chosen as the final model due to its ability to handle multicollinearity (correlation among predictors) by introducing a regularization term (alpha). This helps in reducing model complexity and overfitting, thereby improving generalization to new data. The best alpha parameter obtained from GridSearchCV ensures optimal regularization strength, balancing between bias and variance to enhance model performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Model Explanation: Ridge regression is a linear regression model that incorporates L2 regularization. It adds a penalty term to the ordinary least squares objective function, minimizing the sum of squared residuals plus the regularization term (alpha times the sum of squared coefficients). This regularization helps in shrinking the coefficients towards zero, reducing their variance and improving the model's ability to generalize.\n",
        "\n",
        "*   Feature Importance: In linear models like Ridge regression, feature importance can be inferred from the magnitude of the coefficients after fitting the model. Larger coefficients indicate stronger influence of those features on the predicted outcome. Tools like permutation importance or partial dependence plots can further help visualize and interpret the impact of each feature on the model predictions, providing insights into which variables are most influential in determining the target variable.\n",
        "\n"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project successfully applied Ridge regression to predict stock prices using historical data, demonstrating the effectiveness of machine learning techniques in financial forecasting. Comprehensive data preprocessing ensured high-quality inputs by addressing missing values, correcting data types, managing outliers, and encoding categorical variables, along with scaling numerical features for optimal model performance.\n",
        "\n",
        "Ridge regression was selected to handle multicollinearity and prevent overfitting, with hyperparameter tuning via GridSearchCV identifying an optimal alpha value of 0.1.\n",
        "\n",
        "Exploratory Data Analysis (EDA) provided valuable insights into stock price trends and relationships between variables, with visualizations like line plots and scatter plots aiding in understanding the data patterns and influential factors.\n",
        "\n",
        "In summary, the project effectively utilized machine learning techniques, specifically Ridge regression, to predict stock prices with high accuracy. The rigorous data preprocessing, careful model selection, and thorough evaluation highlighted the robustness of the methodology. Future work could involve exploring additional models or advanced feature engineering techniques to further enhance the predictive capabilities."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}